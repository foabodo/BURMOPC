{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This iPython notebook implements a probabilistic formulation of the ranking principal curve model described\n",
    "# in the Chun-Guo Li et al. paper \"Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves\"\n",
    "# (https://arxiv.org/abs/1402.4542). The model learns to rank 171 countries by four \"quality of life\" (QoL)\n",
    "# indicators drawn from GAPMINDER data (https://www.gapminder.org/data/), following the Zinovyev and Gorban\n",
    "# paper \"Nonlinear Quality of Life Index\" (https://arxiv.org/abs/1008.4063). We compare our inference results\n",
    "# to the ordering of countries presented in Figure 1 of that paper (i.e we perform eyeball validation). More\n",
    "# rigorous validation and model comparison methods are planned for future work. We implement our model using\n",
    "# the probabilistic programming language PyMC (https://www.pymc.io).\n",
    "#\n",
    "# The procedure given in the Li et al. paper uses mathematical optimization to minimize the error between the\n",
    "# reconstruction of feature vectors given by a ranking principal curve and the true/observed feature values.\n",
    "# In this work, we use Bayesian inferece/inverse probability to maximize the probability that the observed\n",
    "# feature values are draws from the posterior predictive distribution over feature vectors derived from a \n",
    "# posterior distirbution over raking principal curves.\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we prepare the functions used for feature endineering. \n",
    "\n",
    "# We use Beta-distributed random variables to model the response/data distribution. Because the support in \n",
    "# PyMC is bounded by (0, 1), when rescaling the features in our data set we set the bounds to be slightly \n",
    "# higher than zero and slightly lower than one.\n",
    "near_zero = 1e-12\n",
    "\n",
    "def min_max_scale(x, min=near_zero, max=1. - near_zero):\n",
    "    x_scaled = (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "    x_scaled = x_scaled * (max - min) + min\n",
    "    return x_scaled\n",
    "\n",
    "# some features (like life expectancy) increase in value as the QoL associated with that feature increases.\n",
    "# Others (like infant morality rate) decrease in value as the associated QoL increases. We reverse the scale\n",
    "# for features of the latter kind to match features of the former kind.\n",
    "def range_reverse(x):\n",
    "    x_reversed = 1. - x\n",
    "    return x_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data set prepared from the GAPMINDER website.\n",
    "gapminder_series = pd.read_csv(\"./data/gapminder_quality_of_life_2005_dataset.csv\", index_col=\"country\")\n",
    "gapminder_data = gapminder_series.values\n",
    "\n",
    "gapminder_data = min_max_scale(gapminder_data)\n",
    "\n",
    "gapminder_data[:, 1] = range_reverse(gapminder_data[:, 1])\n",
    "gapminder_data[:, 2] = range_reverse(gapminder_data[:, 2])\n",
    "\n",
    "# As in the Li et al. paper, we transpose the data matrix to comply with the matrix form of the likelihood function\n",
    "X = np.transpose(gapminder_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define the model prior probability distribution parameters\n",
    "dims, num_scores = X.shape\n",
    "\n",
    "# Initialize the Bezier control points to form a moderately sigmoidal curve shape\n",
    "alpha_prior = np.repeat([.75], dims) + .25 * (rng.random(dims) - .5)\n",
    "p_1_prior = np.repeat([.3125], dims) + .25 * (rng.random(dims) - .5)\n",
    "p_2_prior = np.repeat([.6875], dims) + .25 * (rng.random(dims) - .5)\n",
    "\n",
    "# Use the means across features of a country as priors for their scalar score values.\n",
    "s_prior = np.mean(X, axis=0)\n",
    "\n",
    "# In the mean/variance parameterization od Beta distrubutions, sigmas are tiny. These values\n",
    "# were chosen through trial and error such that the MCMC sampler would initialize at a point in the\n",
    "# search space that yields a valid initial logprob. We also bound the sigmas for the same reason.\n",
    "#\n",
    "# Separately, all points share a common sigma, and all scores share a common sigma as a form of \n",
    "# regularization. Each dimension of the response variable is modeled separately with separate sigmas.\n",
    "# We learn the values of response sigmas because calculating them directly from the data would assume\n",
    "# (incorrectly) that the data are Gaussian-distributed. (*Confusing phrasing?)\n",
    "point_sigma_prior = .1\n",
    "score_sigma_prior = .135\n",
    "X_reconstruction_sigma_prior = np.repeat(.125, dims)\n",
    "\n",
    "sigma_prior_sigma = .1\n",
    "sigma_prior_upper_bound_delta = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define our probabilistic model. Our sigmas are normally-distributed, but bounded as described\n",
    "# earlier. As described in the Li et al. paper, to constrain the principal curve so that it is suitable\n",
    "# for ranking purposes, the anchor points p_0 and p_3 are modeled as a function of another parameter, alpha,\n",
    "# as defined in Proposition 1.\n",
    "with pm.Model() as model:\n",
    "    point_sigma = pm.TruncatedNormal(\n",
    "        name=\"point_sigma\", \n",
    "        mu=point_sigma_prior, \n",
    "        sigma=sigma_prior_sigma, \n",
    "        lower=near_zero, \n",
    "        upper=point_sigma_prior + sigma_prior_upper_bound_delta - near_zero\n",
    "        )\n",
    "    score_sigma = pm.TruncatedNormal(\n",
    "        name=\"score_sigma\", \n",
    "        mu=score_sigma_prior,\n",
    "        sigma=sigma_prior_sigma, \n",
    "        lower=near_zero, \n",
    "        upper=score_sigma_prior + sigma_prior_upper_bound_delta - near_zero\n",
    "        )\n",
    "    X_reconstruction_sigma = pm.TruncatedNormal(\n",
    "        name=\"X_reconstruction_sigma\", \n",
    "        mu=X_reconstruction_sigma_prior,\n",
    "        sigma=sigma_prior_sigma,\n",
    "        lower=near_zero, \n",
    "        upper=X_reconstruction_sigma_prior + sigma_prior_upper_bound_delta - near_zero\n",
    "        )\n",
    "    alpha = pm.Beta(\n",
    "        name=\"alpha\", \n",
    "        mu=alpha_prior, \n",
    "        sigma=point_sigma\n",
    "        )\n",
    "    p_0 = pm.Deterministic(\n",
    "        name=\"p_0\", var=.5 * (1. - alpha)\n",
    "        )\n",
    "    p_1 = pm.Beta(\n",
    "        name=\"p_1\", \n",
    "        mu=p_1_prior, \n",
    "        sigma=point_sigma\n",
    "        )\n",
    "    p_2 = pm.Beta(\n",
    "        name=\"p_2\", \n",
    "        mu=p_2_prior, \n",
    "        sigma=point_sigma\n",
    "        )\n",
    "    p_3 = pm.Deterministic(\n",
    "        name=\"p_3\", var=.5 * (1. + alpha)\n",
    "        )\n",
    "    s = pm.Beta(\n",
    "        name=\"s\", \n",
    "        mu=s_prior, \n",
    "        sigma=score_sigma\n",
    "        )\n",
    "\n",
    "    # Here, we define our likelihood function based on Formulas (15) and (23).\n",
    "    Z = pm.math.stack(\n",
    "        [pt.ones((num_scores,)),\n",
    "         s,\n",
    "         s ** 2.,\n",
    "         s ** 3.],\n",
    "         axis=0\n",
    "    )\n",
    "    M = pt.as_tensor(\n",
    "        [[1., -3.,  3., -1.], \n",
    "         [0.,  3., -6.,  3.],\n",
    "         [0.,  0.,  3., -3.], \n",
    "         [0.,  0.,  0.,  1.]]\n",
    "        )\n",
    "    P = pm.math.stack(\n",
    "        [pt.transpose(p_0), \n",
    "         pt.transpose(p_1), \n",
    "         pt.transpose(p_2), \n",
    "         pt.transpose(p_3)], \n",
    "        axis=1\n",
    "        )\n",
    "    X_reconstruction_mu = pm.math.matmul(pm.math.matmul(P, M), Z)\n",
    "\n",
    "    for i in range(dims):\n",
    "        X_reconstruction = pm.Beta(\n",
    "            name=f\"X_reconstruction_{i}\", \n",
    "            mu=X_reconstruction_mu[i, :], \n",
    "            sigma=X_reconstruction_sigma[i],\n",
    "            observed=X[i, :]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check our model construction using a graphical visualization\n",
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our prior predictive distribution to 1) confirm that our model is formulated as expected, and 2) sanity check\n",
    "# the ability of he sampler to work with the bounds that we defined on our priors.\n",
    "with model:\n",
    "    predictions = pm.sample_prior_predictive(samples=1000, random_seed=rng)\n",
    "\n",
    "az.plot_ppc(predictions, group=\"prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we hit the inference button, making sure to initialize the algorithm at a point in the search space that we have previously\n",
    "# demonstrated to yield a valid log probability calulation result.\n",
    "initvals = {\n",
    "    \"point_sigma\": point_sigma_prior,\n",
    "    \"score_sigma\": score_sigma_prior,\n",
    "    \"X_reconstruction_sigma\": X_reconstruction_sigma_prior,\n",
    "    \"alpha\": alpha_prior,\n",
    "    \"p_1\": p_1_prior,\n",
    "    \"p_2\": p_2_prior,\n",
    "    \"s\": s_prior\n",
    "    }\n",
    "\n",
    "# The pymc sampler has a C implementation that runs on CPU. One alternative sampler that runs on GPU is \"numpyro.\" \n",
    "with model:\n",
    "    idata1 = pm.sample(\n",
    "        nuts_sampler=\"pymc\",\n",
    "        nuts_sampler_kwargs={\"chain_method\": \"vectorized\"},\n",
    "        target_accept=.99,\n",
    "        random_seed=rng,\n",
    "        tune=1000,\n",
    "        draws=1000,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        initvals=initvals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a fit model, lets see how close our reconstructions are to our true data. We can see that the fit is good, but not perfect.\n",
    "# We may get a better fit, and one that is more robust to different shapes of data distributions, by using mixtures of Beta \n",
    "# distributions for each feature dimension, where a Dirichlet prior on mixture weights would allow the optimal number of mixture \n",
    "# components to be learned for any given data set.\n",
    "with model:\n",
    "    idata1 = pm.sample_posterior_predictive(\n",
    "        idata1, extend_inferencedata=True, random_seed=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, and below, we visualize and save the posterior distribution over latent model vairables that we have learned\n",
    "date_time = \"2024-05-22_0947\"\n",
    "az.to_netcdf(idata1, f\"./results/rpc_pymc_{date_time}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_plot = az.plot_ppc(idata1)\n",
    "plt.savefig(f'./results/rpc_pymc_{date_time}_ppc_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_trace_plot = az.plot_trace(idata1, var_names=['alpha'])\n",
    "fig = alpha_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_alpha_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0_trace_plot = az.plot_trace(idata1, var_names=['p_0'])\n",
    "fig = p_0_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_p_0_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1_trace_plot = az.plot_trace(idata1, var_names=['p_1'])\n",
    "fig = p_1_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_p_1_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2_trace_plot = az.plot_trace(idata1, var_names=['p_2'])\n",
    "fig = p_2_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_p_2_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_3_trace_plot = az.plot_trace(idata1, var_names=['p_3'])\n",
    "fig = p_3_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_p_3_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_sigma_trace_plot = az.plot_trace(idata1, var_names=['point_sigma'])\n",
    "fig = point_sigma_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_point_sigma_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sigma_trace_plot = az.plot_trace(idata1, var_names=['score_sigma'])\n",
    "fig = score_sigma_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_score_sigma_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstruction_sigma_trace_plot = az.plot_trace(idata1, var_names=['X_reconstruction_sigma'])\n",
    "fig = X_reconstruction_sigma_trace_plot.flatten()[0].get_figure()\n",
    "fig.savefig(f'./results/rpc_pymc_{date_time}_X_reconstruction_sigma_trace_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we visualize just one country's score distribution at a time. In the cell below, we visualize of all scores in a single plot\n",
    "posterior = idata1.posterior.stack(sample=(\"chain\", \"draw\"))\n",
    "plt.hist(posterior[\"s\"][0], 25, alpha=0.2, color='k')\n",
    "plt.savefig(f'./results/rpc_pymc_{date_time}_s_hist_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_s = idata1.posterior['s']\n",
    "mean_score = posterior_s.mean(('chain', 'draw'))\n",
    "y = np.linspace(0, 1, len(mean_score))\n",
    "hdi = az.hdi(posterior_s).sortby(mean_score)\n",
    "plt.plot(mean_score.sortby(mean_score), y)\n",
    "plt.fill_betweenx(y, hdi['s'].values[:, 0], hdi['s'].values[:, 1], alpha=0.3)\n",
    "plt.savefig(f'{date_time}_scores_hdi_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean = np.mean(posterior[\"s\"], axis=1)\n",
    "gapminder_series.insert(4, \"score\", scores_mean)\n",
    "# gapminder_series[\"score\"] = scores\n",
    "gapminder_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the sorted scores (top and bottom ~5) to get a hint of how good the rankings are.\n",
    "gapminder_series.sort_values(\"score\", inplace=True, ascending=False)\n",
    "gapminder_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our data set out to disk with an added \"score\" column and sorted\n",
    "gapminder_series.to_csv(f\"./data/gapminder_quality_of_life_{date_time}_with_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
